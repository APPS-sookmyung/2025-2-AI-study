# 논문 리뷰: _LLM Collaboration With Multi-Agent Reinforcement Learning_

**링크**: [arXiv:2508.04652](https://arxiv.org/pdf/2508.04652)
**GitHub**: (제공되지 않음)

---

## 1) 연구 배경

대규모 언어 모델(LLM)은 방대한 데이터로 사전 훈련되어 다양한 분야에서 놀라운 능력을 보여주었습니다. 그러나 대부분의 `LLM`은 개별적으로 훈련되어 **협업이나 다중 에이전트 시스템(MAS) 내에서의 조정을 위해 특별히 최적화되지 않았습니다.** 기존 `LLM` 미세 조정(fine-tuning) 프레임워크는 개별적인 보상에 의존하는데, 이는 효과적인 협업을 장려하기 위해 각 에이전트에게 복잡한 보상을 설계해야 하는 어려움이 있습니다.

최근 일부 연구는 여러 `LLM`의 조정을 시도했지만, 대부분 추론 단계에서 프롬프트 수준의 상호작용에 의존합니다. 이러한 방법은 `LLM` 자체를 협업 중심 목표에 맞춰 튜닝하지 않아, 에이전트 간에 **응답 불일치, 잘못된 정보 전파, 비효율적인 통신** 등의 문제를 야기할 수 있습니다. 또한, 효과적인 프롬프트 디자인 자체가 어렵고, 개별적으로 미세 조정하는 방식은 `비정상 환경(non-stationary environment)`에서의 `수렴 보장`이 부족하다는 한계가 있습니다.

> 본 논문은 이러한 도전 과제를 해결하기 위해 `LLM` 협업을 **`협력적 다중 에이전트 강화 학습(MARL)` 문제**로 모델링하고, 이를 **`Decentralized Partially Observable Markov Decision Process (Dec-POMDP)`**로 공식화하여 `LLM` 에이전트들이 고품질의 응답을 효율적으로 생성하도록 효과적인 협력을 배우게 하는 새로운 알고리즘 `MAGRPO`를 제안합니다.

---

## 2) 핵심 기여(contributions)

- **`LLM` 협업의 `MARL` 문제 모델링**: `LLM` 협업을 `협력적 MARL` 문제로 정식화하고, 다수의 `LLM`이 공동의 응답을 생성하기 위해 협력하는 `Dec-POMDP`로 정의합니다. 이는 `LLM`이 `MAS`의 에이전트로서 작동하는 새로운 관점을 제시합니다.
- **`MAGRPO` 알고리즘 개발**: `Group Relative Policy Optimization (GRPO)` 및 `MAPPO`와 같은 최첨단 `LLM RL` 접근 방식과 `MARL` 기법을 기반으로 `Multi-Agent Group Relative Policy Optimization (MAGRPO)` 알고리즘을 개발합니다. 이 알고리즘은 중앙 집중식 그룹-상대 이점(group-relative advantages)을 활용하여 에이전트 협력을 공동으로 최적화하면서도, 각 에이전트의 분산 실행을 유지하여 효율성을 보장합니다.
- **실험적 검증**: `LLM` 글쓰기(요약 및 확장) 및 코딩 협업 작업에 대한 실험을 통해, `MAGRPO`를 사용한 `MAS` 미세 조정이 에이전트가 효과적인 협력을 통해 고품질 응답을 효율적으로 생성하도록 돕는다는 것을 입증합니다.
- **`MARL` 적용의 도전 과제 및 기회 제시**: 기존 `LLM` 협업 접근 방식의 한계를 심층 분석하고, `LLM` 협업에 `MARL` 방법론을 적용할 때 발생하는 개방형 도전 과제와 미래 연구 방향을 제시합니다.

---

## 3) MAGRPO 아키텍처 핵심 구조

`MAGRPO`는 다중 에이전트 시스템(MAS)에서 `LLM` 에이전트들을 공동으로 훈련시키면서 분산 실행을 유지하는 알고리즘입니다. 이 방법은 `Decentralized Partially Observable Markov Decision Process (Dec-POMDP)`를 기반으로 합니다.

### 주요 구성요소 및 `Dec-POMDP` 정의

`LLM Dec-POMDP`는 다음과 같은 튜플 $\langle I, S, \{O_i\}, \{A_i\}, R, T, H \rangle$로 정의됩니다.

- $I = \{1, \ldots, n\}$: $n$개의 `LLM` 에이전트 집합.
- $S$: 전체 전역 상태 공간. `t` 시점의 상태 $s_t = (s^{\text{acc}}_t, s^{\text{usr}}_t)$는 모델에서 접근 가능한 부분 $s^{\text{acc}}_t$와 접근 불가능한 사용자 상태 $s^{\text{usr}}_t$로 구성됩니다.
- $O_i$: 에이전트 $i$의 관측 공간. 로컬 관측 $o_{i,t}$는 자연어 지시(프롬프트)로 구성되며, $s_t$의 부분적이고 노이즈가 있는 뷰를 제공합니다.
- $A_i$: 에이전트 $i$의 행동 공간. 로컬 행동 $a_{i,t}$는 주어진 프롬프트에 대한 자연어 응답입니다.
- $R : S^{\text{acc}} \times A \to \mathbb{R}$: 사전 정의된 규칙 또는 사전 훈련된 보상 모델을 통해 구현되는 **공동 보상 함수**입니다. `t` 시점의 공동 보상 $r_t$는 $s^{\text{acc}}_t$와 에이전트의 공동 행동 $a_t = \{a_{1,t}, \ldots, a_{n,t}\}$에 의해 결정됩니다.
- $T : S \times A \to \Delta(S)$: 기저 확률적 상태 전이 함수입니다. $a_t$는 새로운 상태 $s_{t+1} \sim T(\cdot|s_t, a_t)$로의 전환을 유도합니다.
- $H$: 에피소드 시간 한계 (대화 턴 제한).

각 에이전트는 상태를 직접 관측할 수 없으므로, 로컬 관측-행동 이력 $h_{i,t} = \{o_{i,0}, a_{i,0}, \ldots, o_{i,t}\}$을 유지하여 상태 정보를 추론합니다. 목표는 기대 누적 보상을 최대화하는 공동 정책 $\pi^* = \{\pi^*_1, \ldots, \pi^*_n\} = \arg\max_{\pi} \mathbb{E}_{\pi} \left[ \sum_{t=0}^{H-1} R(s^{\text{acc}}_t, a_t) \right]$을 찾는 것입니다.

### MAGRPO 알고리즘의 대화 흐름 (훈련 과정)

`MAGRPO` 알고리즘은 다음과 같은 절차로 `LLM` 에이전트들을 공동으로 훈련합니다.

1.  **데이터셋 및 초기화**: 태스크 정보($\mathcal{D}$)를 포함하는 데이터셋과 $n$개의 사전 훈련된 `LLM`($\{\pi_{\theta_1}, \ldots, \pi_{\theta_n}\}$)이 주어집니다.
2.  **에피소드 진행**:
    - 각 에피소드마다 태스크가 $\mathcal{D}$에서 샘플링되어 초기 관측 $o_0 = \{o_{1,0}, \ldots, o_{n,0}\}$ 및 이력 $h_0 = \{h_{1,0}, \ldots, h_{n,0}\}$이 구성됩니다.
    - **다중 턴 상호작용**: 각 턴 `t`에서:
      - 각 에이전트 $i$는 자신의 이력 $h^G_{i,t}$를 기반으로 정책 $\pi_{\theta_i}$에 따라 $G$개의 응답 그룹 $a^G_{i,t} = \{a^{(1)}_{i,t}, \ldots, a^{(G)}_{i,t}\}$을 생성합니다.
      - 에이전트들의 행동은 $a^G_t = \{a^G_{1,t}, \ldots, a^G_{n,t}\}$으로 취합됩니다.
      - 에이전트들은 시스템으로부터 공동 보상 $r^G_t$를 받습니다.
      - 새로운 관측 $o^G_{i,t+1}$을 받고, 이력을 $h^G_{i,t+1} \leftarrow \{h^G_{i,t}, a^G_{i,t}, o^G_{i,t+1}\}$로 업데이트합니다.
    - 이 과정은 턴 한계 $H$에 도달할 때까지 계속됩니다.
3.  **정책 최적화**: 에피소드가 끝날 때 각 턴 `t`에 대해:
    - **그룹별 수익 계산**: 각 그룹 `g`의 총 수익 $R^{(g)}_t = \sum_{\tau=t}^{H-1} r^{(g)}_\tau$를 계산합니다.
    - **그룹 상대 이점 추정**: 단일 롤아웃(`rollout`)으로 인한 높은 분산을 안정화하기 위해, 그룹 내 Monte Carlo 샘플($\{R^{(1)}_t, \ldots, R^{(G)}_t\}$)을 평균하여 현재 상태의 기대 수익을 추정하고, 이를 기반으로 각 공동 행동의 이점($\tilde{A}^{(g)}_t$)을 계산합니다.
      $$
      \tilde{A}^{(g)}_t = \frac{R^{(g)}_t - \frac{1}{G}\sum_{g=1}^G R^{(g)}_t}{\sigma(R^G_t)}
      $$
      여기서 $\sigma(R^G_t)$는 그룹 수익의 표준 편차입니다.
    - **정책 업데이트**: 이 중앙 집중식 이점 값은 각 에이전트 $i$의 정책 $\pi_i$ (파라미터 $\theta_i$)를 업데이트하는 데 사용됩니다.
      $$
      J(\theta_i) = \mathbb{E}_{o_0 \sim \mathcal{D}, h^G \sim \pi_{\theta, \text{old}}} \left[ \frac{1}{|\mathcal{B}|} \frac{1}{|G|} \sum_{h^G_i \in \mathcal{B}} \sum_{g \in G} \min \left( \rho^{(g)}_{i,t} \tilde{A}^{(g)}_t, \varepsilon \text{-clip}(\rho^{(g)}_{i,t}) \tilde{A}^{(g)}_t \right) \right]
      $$
      여기서 $\rho^{(g)}_{i,t} = \frac{\pi_{\theta_i}(a^{(g)}_{i,t}|h^{(g)}_{i,t})}{\pi_{\theta_{i, \text{old}}}(a^{(g)}_{i,t}|h^{(g)}_{i,t})}$는 중요도 샘플링 비율입니다.
4.  **반복**: 이 에피소드 반복 과정을 통해 `LLM` 정책들이 공동으로 최적화됩니다.

이 `MAGRPO` 접근 방식은 `LLM` 에이전트들이 자연어 기반의 복잡한 협업 작업을 효율적이고 안정적으로 수행할 수 있도록 훈련하는 강력한 프레임워크를 제공합니다.

---

## 4) 실험 및 사례

`MAGRPO`는 `LLM` 글쓰기 및 코딩 협업 작업에서 평가되었습니다.

### 실험 시나리오

| 시나리오        | 구성                                        | 설명                                                                                                                                                           | 평가 지표 (요약)                                                                                 |
| :-------------- | :------------------------------------------ | :------------------------------------------------------------------------------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------- |
| **글쓰기 협업** | `Qwen3-1.7B` 에이전트 2명                   |                                                                                                                                                                | 구조적 건전성 (길이/고유 단어 비율), 스타일 일관성 (Jaccard 유사도), 논리적 일관성 (전환어 사용) |
| TLDR 요약       | `핵심 아이디어` 생성기 + `상세 요약` 생성기 | Reddit 게시물에서 핵심(TLDR) 요약과 상세 요약을 독립적으로 생성.                                                                                               |                                                                                                  |
| arXiv 확장      | `배경/동기` 작성기 + `방법/실험` 작성기     | arXiv 논문 초록을 기반으로 서론 섹션 확장. 각 에이전트가 다른 부분을 담당하여 일관된 글을 생성.                                                                |                                                                                                  |
| **코딩 협업**   | `Qwen2.5-Coder-3B` 에이전트 2명             | `HumanEval (HE)` 및 `CoopHumanEval (CHE)` 데이터셋 사용. 한 에이전트가 `보조 함수 (auxiliary function)`를, 다른 에이전트가 `메인 함수 (main function)`를 생성. | 구조적 무결성, 구문 정확성, 테스트 통과율, 협업 품질 (메인 함수가 보조 함수를 활용하는지 여부)   |

### baselines

- **`Single Model`**: 단일 `LLM`이 전체 작업을 수행 (비교 대상).
- **`Parallel Generation (Naive Concatenation)`**: 여러 `LLM`이 독립적으로 작업 후 결과물을 단순히 연결. 통신 없음.
- **`Sequential Generation (Sequential Pipeline)`**: 한 `LLM`의 결과물이 다음 `LLM`의 입력으로 제공되는 단방향 통신.
- **`One-Round Discussion`**: 초기 응답 후, 서로의 응답을 참조하여 한 번 더 수정하는 양방향 통신 (fine-tuning 없음).

### 주요 결과

- **글쓰기 협업 (`TLDR` 요약, `arXiv` 확장)**:
  - `MAGRPO`는 단일 `LLM` 및 프롬프트 기반 `multi-agent baselines`에 비해 **최대 3배 빠른 속도**로 높은 품질의 응답을 생성했습니다.
  - `MAGRPO`는 구조, 스타일 일관성, 논리적 응집력 측면에서 가장 높은 `Total Return`을 달성했습니다 (Table 1, Figure 2).
  - 훈련을 통해 에이전트들이 점진적으로 협력하여 응집력 있고 일관된 콘텐츠를 생성하는 것을 확인했습니다.
- **코딩 협업 (`HumanEval`, `CoopHumanEval`)**:
  - `MAGRPO`는 단일 턴 및 다중 턴 설정 모두에서 모든 `baseline` 모델을 능가했습니다. 특히 `CHE` 데이터셋에서 더 높은 전반적인 보상과 낮은 분산을 보였습니다 (Table 2, Figure 3).
  - 다중 턴 `MAGRPO`는 외부 모델(`Claude-Sonnet-4`)의 피드백을 통합하여 초기에는 어려움을 겪었지만, 점진적으로 학습하여 단일 턴 훈련보다 더 높은 성능을 달성했습니다. 이는 **신뢰할 수 있는 외부 제안이 있을 때 에이전트가 피드백을 활용하여 응답 품질을 향상**할 수 있음을 보여줍니다.
  - `pass@k`, `acc@k`, `coop@k`와 같은 추가 지표에서도 `MAGRPO`가 `baseline`을 크게 앞서는 결과를 보였습니다 (Table 3).
- **협업 스키마의 출현**: 훈련을 통해 **`Fallback` (메인 에이전트가 보조 함수 오류에 대비), `Decorator` (보조 함수 결과에 추가 기능 부여), `Coordinator` (메인 에이전트가 작업을 분해하고 보조 에이전트에 할당), `Strategy Filter` (보조 에이전트가 특정 로직 분기 필터 역할)**와 같은 다양하고 복잡한 협업 스키마가 자연스럽게 나타났습니다. 이는 `MAGRPO`가 에이전트들에게 유연한 협력 방법을 학습시킨다는 것을 의미합니다.

---

## 5) 응용 프로그램 예시

`MAGRPO`와 같은 `MARL` 기반 `LLM` 협업은 다음과 같은 분야에서 활용될 수 있습니다.

1.  **복잡한 콘텐츠 생성 및 수정**: 대규모 문서, 기술 보고서, 창의적인 스토리 등을 여러 `LLM` 에이전트가 공동으로 작성하고 상호 검토 및 수정하는 시스템. 각 에이전트가 특정 장르, 스타일, 또는 내용 전문성을 가질 수 있습니다.
2.  **대규모 소프트웨어 개발 프로젝트**: 여러 `LLM` 개발자 에이전트가 기능별, 모듈별로 역할을 분담하여 코드를 작성, 검증, 디버깅하고, 통합하는 자동화된 개발 환경. 특히, `auxiliary function`과 `main function` 간의 조율처럼 세분화된 협업이 가능합니다.
3.  **심층 지식 탐색 및 분석 시스템**: 여러 `LLM` 에이전트가 방대한 데이터를 탐색하고, 정보를 요약하며, 비판적으로 평가하여 복잡한 질문에 대한 심층적이고 신뢰성 높은 답변을 생성하거나 특정 주제에 대한 종합적인 분석 보고서를 만듭니다.
4.  **다중 모달 `LLM` 시스템**: 텍스트 생성 에이전트, 이미지 생성 에이전트, 오디오 분석 에이전트 등이 협력하여 복잡한 `multi-modal` 창작 또는 분석 작업을 수행합니다.
5.  **동적이고 적응적인 의사 결정 시스템**: 특정 시나리오에 따라 에이전트의 역할과 협업 전략을 동적으로 변경하면서 최적의 의사 결정을 도출하는 시스템. 예를 들어, 재난 대응 또는 금융 시장 분석에서 활용될 수 있습니다.

---

## 6) 해석 및 한계

### 해석

- **`LLM` 협업의 새로운 패러다임 제시**: `MAGRPO`는 단순히 프롬프트 지시에 의존하는 것을 넘어, `LLM`이 **강화 학습을 통해 실제 협업 전략을 능동적으로 학습**하도록 유도하는 강력한 프레임워크를 제공합니다. 이는 `LLM`을 단일 에이전트에서 복잡한 문제를 해결하는 `팀`으로 진화시키는 중요한 단계입니다.
- **효율성과 품질의 동시 달성**: `MAGRPO`는 글쓰기 및 코딩 작업에서 기존 `baseline`보다 높은 품질의 결과물을 더 효율적으로 생성하며, 특히 `Dec-POMDP` 모델링을 통해 복잡한 상황에서도 에이전트 간의 **응집력 있는 행동**을 유도합니다.
- **자연스러운 협업 스키마의 출현**: 명시적인 프로그래밍 없이도 `LLM`들이 훈련을 통해 `fallback`, `decorator`, `coordinator`, `strategy filter`와 같은 **다양하고 정교한 협업 패턴을 스스로 발견**하는 능력은 `MARL`의 잠재력을 강력하게 보여줍니다.
- **`LLM` 추론 가속화 및 로버스트니스 향상**: 대규모 단일 모델의 부담을 분산하고, 각 에이전트가 특정 하위 태스크에 집중하도록 하여 **추론 효율성을 높이고, 모듈화된 추론을 통해 시스템의 로버스트니스**를 향상시킬 수 있는 가능성을 열어줍니다.

### 한계

- **동질적 에이전트 중심**: 연구는 주로 유사한 기능을 가진 동질적 에이전트(예: 모두 `Python` 함수를 생성하는 에이전트) 간의 협업에 초점을 맞춥니다. 다양한 기능과 역할을 가진 이질적 에이전트 간의 복잡한 협업에 대한 탐구는 제한적입니다.
- **훈련 데이터 및 모델 규모의 제약**: 계산 제약으로 인해 비교적 작은 규모의 `LLM`을 사용하여 제한된 데이터셋으로 훈련을 진행했습니다. 대규모 실제 프로젝트에서는 더 다양하고 복잡한 협업 스키마가 나타날 수 있습니다.
- **보상 모델의 단순성**: 현재 보상 모델은 비교적 단순하며, 이는 좁은 보상 신호와 잠재적인 `reward hacking` 문제로 이어질 수 있습니다. 인간의 선호도에 더 잘 부합하는, 더 표현력이 풍부하고 세분화된 보상 모델(예: 다중 측면 보상, 프로세스 감독 보상)의 설계가 중요합니다.
- **외부 피드백 해석의 어려움**: 특히 소규모 모델의 경우, 외부 에이전트로부터 제공되는 추상적인 피드백이나 제안을 효과적으로 해석하고 훈련에 통합하는 데 어려움을 겪을 수 있습니다. 이는 더 높은 추론 능력을 가진 `LLM`의 필요성을 시사합니다.
- **`Dec-POMDP`의 복잡성**: `Dec-POMDP`는 이론적으로 강력하지만, 실제 대규모 `LLM` 환경에 적용할 때 상태 및 행동 공간의 폭발적인 증가로 인한 계산 복잡성 문제를 완전히 해결하지 못할 수 있습니다.

---

## 7) 향후 과제

- **이질적 에이전트 협업 탐구**: 다양한 기능(`텍스트`, `코드`, `이미지`, `데이터 분석` 등)을 가진 `LLM` 에이전트들이 복잡한 작업을 위해 어떻게 효과적으로 협력할 수 있는지 연구하여, 더욱 일반적인 `MAS` 구성에 적용 가능하도록 합니다.
- **스케일업 및 실제 환경 적용**: `MAGRPO`를 더 큰 규모의 `LLM`과 실제처럼 복잡한 데이터셋, 예를 들어 여러 파일과 모듈을 포함하는 대규모 소프트웨어 프로젝트에 적용하여, 새로운 협업 스키마와 최적화 기법을 발견합니다.
- **고급 보상 모델 설계**: `multi-aspect rewards`, `process-supervised rewards`, `human feedback` 및 `Constitutional AI`와 같은 기술을 활용하여, `LLM` 에이전트의 협력 행동을 인간의 선호도와 더 정확하게 일치시키는 정교하고 세분화된 보상 모델을 개발합니다.
- **메모리 기반 에이전트 확장**: 에이전트가 장기 기억을 활용하여 과거의 경험, 지식, 그리고 다른 에이전트와의 상호 작용 기록을 기반으로 더 효율적이고 정교한 협업 전략을 개발하도록 훈련합니다.
- **안정적인 제어 및 예외 처리 메커니즘**: `MARL` 훈련 과정의 불안정성 문제를 해결하고, 에이전트 간의 상호 작용에서 발생할 수 있는 오류나 예상치 못한 상황에 대한 강력한 `제어 메커니즘` 및 `예외 처리` 방안을 연구합니다.
- **실시간 상호작용 및 툴 연동 강화**: `MAGRPO` 에이전트가 외부 툴(API, 데이터베이스 등)과 실시간으로 상호작용하고, 이를 통해 문제 해결 능력을 확장하며 실용성을 강화하는 방법을 모색합니다.

---

## 8) 총평

"LLM Collaboration With Multi-Agent Reinforcement Learning" 논문은 `LLM`의 협업 능력을 근본적으로 향상시키기 위한 **`MARL` 기반의 선구적인 접근 방식**을 제시합니다. `Dec-POMDP`를 통해 `LLM` 협업을 정식화하고, `MAGRPO` 알고리즘을 통해 **중앙 집중식 훈련의 이점과 분산 실행의 효율성**을 성공적으로 결합함으로써 `LLM` 에이전트들이 복잡한 글쓰기 및 코딩 작업에서 **고품질의 결과물을 효율적으로 생성하고, 다양한 협업 스키마를 스스로 학습**할 수 있음을 입증했습니다.

이 연구는 `LLM`을 단순히 개별적인 질의응답 도구가 아닌, 서로 학습하고 협력하는 지능형 `에이전트 팀`으로 발전시키는 데 중요한 아이디어를 제공합니다. 이는 `LLM`이 미래의 `복잡한 소프트웨어 시스템`의 핵심 구성 요소로서 **보다 확장 가능하고, 견고하며, 인간과 긴밀하게 협력하는 존재**로 진화할 가능성을 보여주며, `MARL` 기반 `LLM` 협업이라는 새로운 연구 분야를 강력하게 개척하고 있습니다.
