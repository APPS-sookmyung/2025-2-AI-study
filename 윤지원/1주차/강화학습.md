# 강화학습 기반 멀티에이전트 RAG 시스템 최신 동향

2025년은 **강화학습(Reinforcement Learning)**이 RAG 시스템에 본격적으로 적용되기 시작, outcome-based supervision에서 process-level supervision으로 패러다임이 전환되고 있다.

---

## 1. MMOA-RAG: Multi-Module joint Optimization Algorithm

### 1.1 핵심 개념

RAG 파이프라인을 협력적 멀티에이전트 강화학습(Co-MARL) 문제로 모델링하는 혁신적 접근법이다.

#### 아키텍처 구성

```
Query Rewriter → Retriever → Selector → Generator
```

각 모듈이 개별 RL 에이전트**로 작동하되, **공유된 보상(F1 score)\*\*을 최대화하기 위해 협력한다.

#### 핵심 메커니즘: MAPPO (Multi-Agent PPO)

- Global Critic Model: 모든 에이전트의 행동을 중앙에서 평가한다
- Shared Reward Mechanism: 최종 답변의 F1 점수를 공통 목표로 설정한다
- Centralized Training, Decentralized Execution: 훈련 시 중앙집권적, 실행 시 분산적이다

### 1.2 혁신점

- 전체 RAG 파이프라인의 종단간(End-to-End) 최적화를 강화학습으로 구현함
- 기존의 모듈별 독립 최적화의 한계를 극복함
- 2대의 A800 서버 (각각 8GPU × 80GB)를 활용한 실시간 학습 환경 구축

### 1.3 성능 및 한계

- 다양한 QA 데이터셋에서 기존 baseline 대비 우수한 성능 달성
- Out-of-Domain 실험에서 뛰어난 일반화 능력 입증
- 한계: 높은 컴퓨팅 비용과 실시간 검색 모델 배치의 복잡성

---

## 2. RAG-Gym: Systematic Agent Optimization Framework

### 2.1 핵심 개념

**RAG-Gym**은 **nested MDP(Markov Decision Process)**를 활용하여 정보 검색 에이전트를 체계적으로 최적화하는 종합 플랫폼이다.

#### 3차원 최적화 접근법

1. **Prompt Engineering**: Re²Search (Reasoning, Reflection, Search) 아키텍처
2. **Actor Tuning**: DPO(Direct Preference Optimization) 기반 에이전트 조정
3. **Critic Training**: 과정 수준의 보상 모델링

#### MDP 구조화

```
Outer-layer MDP: 검색-환경 상호작용 관리
Inner-layer MDP: 각 검색 단계별 세부 의사결정
```

### 2.2 Re²Search++ 에이전트

**최적화된 통합 에이전트**로 다음 특징을 보임:

- **Search-R1, R1-Searcher 대비 3.2%~11.6% 상대적 F1 향상**
- \*\*Out-of-domain 데이터에서 특히 강력한 성능
- 수천 개의 훈련 질문이 필요한 기존 방식 대비 효율성 확보

### 2.3 Process Supervision의 중요성

- Outcome-based supervision의 한계: 중간 검색 행동의 최적화 부족
- Process-level supervision: 각 검색 단계에 대한 세밀한 지도 제공
- Retrieved token masking: RL 손실 계산 시 검색된 토큰을 마스킹하여 훈련 안정화

---

## 3. M-RAG: Multi-Partition Reinforcement Learning

### 3.1 핵심 아이디어

M-RAG는 데이터베이스를 다중 파티션으로 구성하고, 각 파티션을 기본 RAG 실행 단위로 활용하는 멀티에이전트 강화학습 프레임워크이다.

#### 혁신적 접근

- 전체 데이터베이스 통합 검색의 한계 극복
- 파티션별 전문화로 노이즈 감소 및 핵심 정보 집중
- 3개 언어 생성 태스크에서 일관된 성능 향상 달성

---

## 4. Domain-Specific RL Optimization

### 4.1 FAQ 기반 도메인 챗봇 최적화

실용적 강화학습 적용 사례로 다음과 같은 메커니즘을 구현함:

#### Policy 기반 토큰 최적화

```
State: 현재 질의 + 이전 질의 + 행동 이력
Action: [FETCH] 또는 [NO_FETCH]
Reward: GPT-4 기반 응답 평가 점수
```

#### 주요 구성요소

1. Domain-specific Embedding Model: infoNCE loss 기반 훈련
2. Policy Network: BERT 또는 GPT-2 기반 의사결정 모델
3. GPT-4 Reward Model: 응답 품질 자동 평가
4. Similarity Threshold: OOD 질의 탐지 메커니즘

### 4.2 성과 및 의의

- 167,885 토큰 → 대폭 절약: 구체적 수치는 논문마다 상이
- 비용 효율성과 정확도의 균형 달성
- Follow-up 질의 패턴 학습\*\*을 통한 맥락 재사용 최적화

---

## 5. 최신 기술 동향 및 핵심 인사이트

### 5.1 System 1 vs System 2 사고 체계

**인지과학적 접근**을 통해 RAG 시스템을 분류함:

#### System 1 (Fast Thinking): Predefined RAG

- **즉각적이고 직관적인** 검색-생성 과정
- 프롬프트 엔지니어링 기반 구현

#### System 2 (Slow Thinking): Agentic RAG

- **신중하고 반성적인** 멀티턴 추론 과정
- 강화학습 기반 최적화 구현

### 5.2 Large Reasoning Models (LRMs) 활용

**DeepSeek-R1, OpenAI o1** 등 대규모 추론 모델을 활용한 접근법:

- **본질적 강한 추론 능력**을 RAG에 직접 적용
- **프롬프트 기반 agentic 전략**으로 검색 타이밍과 방법 결정
- **대규모 강화학습** 기법으로 훈련된 모델 활용

### 5.3 Outcome vs Process Supervision 패러다임 전환

#### 기존 Outcome-based 접근법의 한계

- **최종 결과만 평가**하여 중간 과정의 최적화 부족
- **수천 개의 훈련 질문** 필요로 비효율적
- **도메인 외 일반화** 능력 제한

#### Process-level Supervision의 장점

- **각 검색 단계별 세밀한 지도** 제공
- **중간 행동의 품질 평가**로 전체 성능 향상
- **적은 훈련 데이터로도 효과적** 학습 가능
